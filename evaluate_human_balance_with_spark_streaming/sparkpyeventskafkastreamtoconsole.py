from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructField, StructType, StringType, FloatType


# create a schema that matches the risk score events that are generated by the STEDI application
stediEventSchema = StructType([
    StructField("customer", StringType()),
    StructField("score", FloatType()),
    StructField("riskDate", StringType())
])

spark = SparkSession.builder.appName("risk-score").getOrCreate()
spark.sparkContext.setLogLevel('WARN')

stediRawStreamingDF = spark                          \
    .readStream                                          \
    .format("kafka")                                     \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "stedi-events")                  \
    .option("startingOffsets", "earliest")\
    .load()

# cast the key and value columns into string types
stediStreamingDF = stediRawStreamingDF.selectExpr("cast(key as string) key", "cast(value as string) value")

# note that we don't need to unbase64 our data, as it wasn't binary encoded to start with
# we only have to unpack the JSON schema using our STEDI schema
# this should create fields like this:
# +------------+-----+-----------+
# |    customer|score| riskDate  |
# +------------+-----+-----------+
# |"sam@tes"...| -1.4| 2020-09...|
# +------------+-----+-----------+
stediStreamingDF.withColumn("value", from_json("value", stediEventSchema)) \
    .select(
#     col("key"), - this is the key for this row
#     col("value"), - this is the JSON containing the data for this row
    col('value.customer'),
    col('value.score'),
    col('value.riskDate'),
).createOrReplaceTempView("CustomerRisk")

# then, we just select the email and the risk score from that temporary view, and output it to the console
customerRiskStreamingDF = spark.sql("select customer, score from CustomerRisk").writeStream.outputMode("append").format("console").start().awaitTermination()


# Run the python script by running the command from the terminal:
# /home/workspace/submit-event-kafka-streaming.sh
# Verify the data looks correct 